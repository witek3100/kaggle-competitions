{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:59:59.503628Z","iopub.execute_input":"2023-08-18T07:59:59.504077Z","iopub.status.idle":"2023-08-18T08:00:00.229557Z","shell.execute_reply.started":"2023-08-18T07:59:59.504041Z","shell.execute_reply":"2023-08-18T08:00:00.227458Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_train.drop(columns=['location', 'keyword'], inplace=True)\n\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\ndf_test.drop(columns=['location', 'keyword'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:00.232823Z","iopub.execute_input":"2023-08-18T08:00:00.233332Z","iopub.status.idle":"2023-08-18T08:00:00.349808Z","shell.execute_reply.started":"2023-08-18T08:00:00.233265Z","shell.execute_reply":"2023-08-18T08:00:00.348425Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"\n                           u\"\\U0001F300-\\U0001F5FF\"\n                           u\"\\U0001F680-\\U0001F6FF\"\n                           u\"\\U0001F1E0-\\U0001F1FF\"\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_url(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n\n    return text\n\ndef remove_html(text):\n    text = re.sub(r'<.*?>',' ',text)\n    return text\n\ndef remove_mentions(text):\n    text = re.sub('@\\S+', '', text)\n    return text\n\ndf_train['text'] = df_train['text'].apply(lambda x : remove_emoji(x))\ndf_train['text'] = df_train['text'].apply(lambda x : remove_url(x))\ndf_train['text'] = df_train['text'].apply(lambda x : seperate_alphanumeric(x))\ndf_train['text'] = df_train['text'].apply(lambda x : decontraction(x))\ndf_train['text'] = df_train['text'].apply(lambda x : remove_html(x))\ndf_train['text'] = df_train['text'].apply(lambda x : remove_mentions(x))\ndf_train['text'] = df_train['text'].apply(lambda x : x.lower())\n\ndf_test['text'] = df_test['text'].apply(lambda x : remove_emoji(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_url(x))\ndf_test['text'] = df_test['text'].apply(lambda x : seperate_alphanumeric(x))\ndf_test['text'] = df_test['text'].apply(lambda x : decontraction(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_html(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_mentions(x))\ndf_test['text'] = df_test['text'].apply(lambda x : x.lower())","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:00.354373Z","iopub.execute_input":"2023-08-18T08:00:00.354772Z","iopub.status.idle":"2023-08-18T08:00:01.140692Z","shell.execute_reply.started":"2023-08-18T08:00:00.354741Z","shell.execute_reply":"2023-08-18T08:00:01.139479Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"text = pd.concat([df_train['text'], df_test['text']], axis=0)\n\ntfidf_vectorizer = TfidfVectorizer(\n    min_df=2,\n    stop_words='english',\n    ngram_range=(1, 2)\n)\n\nvectors = tfidf_vectorizer.fit_transform(\n    text\n)\n\ntrain_target = df_train['target']\ntest_ids = df_test['id']","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:01.143668Z","iopub.execute_input":"2023-08-18T08:00:01.144063Z","iopub.status.idle":"2023-08-18T08:00:01.765236Z","shell.execute_reply.started":"2023-08-18T08:00:01.144028Z","shell.execute_reply":"2023-08-18T08:00:01.764160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"vectors = pd.DataFrame(vectors.toarray())\n\ntrain = vectors.iloc[:7613, :]\ntest = vectors.iloc[7613:, :]\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:01.766716Z","iopub.execute_input":"2023-08-18T08:00:01.767495Z","iopub.status.idle":"2023-08-18T08:00:02.261828Z","shell.execute_reply.started":"2023-08-18T08:00:01.767455Z","shell.execute_reply":"2023-08-18T08:00:02.260438Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Stacking classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:02.263545Z","iopub.execute_input":"2023-08-18T08:00:02.264044Z","iopub.status.idle":"2023-08-18T08:00:02.835227Z","shell.execute_reply.started":"2023-08-18T08:00:02.263993Z","shell.execute_reply":"2023-08-18T08:00:02.833824Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Splitting train set for two separate sets ","metadata":{}},{"cell_type":"code","source":"train1 = train.iloc[:5000, :]\ntrain2 = train.iloc[5000:, :]\ntrain_target1 = train_target[:5000]\ntrain_target2 = train_target[5000:]","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:02.836889Z","iopub.execute_input":"2023-08-18T08:00:02.838269Z","iopub.status.idle":"2023-08-18T08:00:02.845479Z","shell.execute_reply.started":"2023-08-18T08:00:02.838228Z","shell.execute_reply":"2023-08-18T08:00:02.844135Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Training first layer of classifiers - all of them must be able to return probability","metadata":{}},{"cell_type":"markdown","source":"#### Multinomial Naive Bayes","metadata":{}},{"cell_type":"code","source":"mnb = MultinomialNB()\n\nparam_grid = {\n    'alpha': [0.1, 0.5, 1.0],\n    'fit_prior': [True, False]\n}\n\ngrid_search = GridSearchCV(mnb, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train1, train_target1)\nmnb = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:02.847427Z","iopub.execute_input":"2023-08-18T08:00:02.848237Z","iopub.status.idle":"2023-08-18T08:00:29.873468Z","shell.execute_reply.started":"2023-08-18T08:00:02.848188Z","shell.execute_reply":"2023-08-18T08:00:29.871645Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Bernoulli Naive Bayes","metadata":{}},{"cell_type":"code","source":"bnb = BernoulliNB()\n\nparam_grid = {\n    'alpha': [0.1, 0.5, 1.0],\n    'binarize': [0.0, 0.5, 1.0],\n    'fit_prior': [True, False]\n}\n\ngrid_search = GridSearchCV(bnb, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train1, train_target1)\nbnb = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:00:29.876097Z","iopub.execute_input":"2023-08-18T08:00:29.877204Z","iopub.status.idle":"2023-08-18T08:02:36.690363Z","shell.execute_reply.started":"2023-08-18T08:00:29.877137Z","shell.execute_reply":"2023-08-18T08:02:36.688545Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Random Forest","metadata":{}},{"cell_type":"code","source":"forest = RandomForestClassifier()\n\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(forest, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train1, train_target1)\nforest = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:02:36.696585Z","iopub.execute_input":"2023-08-18T08:02:36.697866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logistic regression","metadata":{}},{"cell_type":"code","source":"log_clf = LogisticRegression()\n\nparam_grid = {\n    'penalty': ['l1', 'l2'],\n    'C': [0.01, 0.1, 1, 10, 100],\n    'solver': ['liblinear', 'saga'],\n    'max_iter': [100, 200, 300]\n}\n\ngrid_search = GridSearchCV(log_clf, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train1, train_target1)\nlog_clf = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### KNeighbors Classifier","metadata":{}},{"cell_type":"code","source":"knc = KNeighborsClassifier()\n\nparam_grid = {\n    'n_neighbors': [3, 5, 7, 9],\n    'weights': ['uniform', 'distance'],\n    'p': [1, 2],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\n\ngrid_search = GridSearchCV(knc, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train1, train_target1)\nknc = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating new train set for second layer (blender) using predictions from first layer classifiers","metadata":{}},{"cell_type":"code","source":"predictions_mnb = pd.DataFrame(mnb.predict_proba(train2))\npredictions_mnb.set_index(train2.index, inplace=True)\n\npredictions_bnb = pd.DataFrame(bnb.predict_proba(train2))\npredictions_bnb.set_index(train2.index, inplace=True)\n\npredictions_forest = pd.DataFrame(forest.predict_proba(train2))\npredictions_forest.set_index(train2.index, inplace=True)\n\npredictions_log = pd.DataFrame(log_clf.predict_proba(train2))\npredictions_log.set_index(train2.index, inplace=True)\n\npredictions_knc = pd.DataFrame(knc.predict_proba(train2))\npredictions_knc.set_index(train2.index, inplace=True)\n\ntrain_blender = pd.DataFrame({\n    'mnb': predictions_mnb[1],\n    'bnb': predictions_bnb[1],\n    'forest': predictions_forest[1],\n    'log': predictions_log[1],\n    'knc': predictions_knc[1]\n}, index=train2.index)\n\ntrain_blender","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training blender","metadata":{}},{"cell_type":"code","source":"svc = SVC()\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n    'gamma': ['scale', 'auto', 0.1, 1]\n}\n\ngrid_search = GridSearchCV(svc, param_grid, cv=5, scoring='f1')\ngrid_search.fit(train_blender, train_target2)\n\nblender = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Final predictions for test set","metadata":{}},{"cell_type":"code","source":"final_predictions = blender.predict(\n    pd.DataFrame({\n        'mnb': [one_proba for zero_proba, one_proba in mnb.predict_proba(test)],\n        'bnb': [one_proba for zero_proba, one_proba in bnb.predict_proba(test)],\n        'forest': [one_proba for zero_proba, one_proba in forest.predict_proba(test)],\n        'log': [one_proba for zero_proba, one_proba in log_clf.predict_proba(test)],\n        'knc': [one_proba for zero_proba, one_proba in knc.predict_proba(test)],\n    }, index=test_ids)\n)\nfinal_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_predictions = pd.DataFrame({'target': final_predictions}, index=test_ids)\nfinal_predictions.to_csv('stacking-classifier.csv')\nfinal_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
